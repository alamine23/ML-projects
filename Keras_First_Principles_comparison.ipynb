{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iatDomnSeRZ3",
        "outputId": "3f71575b-63c3-4f50-9a3a-fe5c73c2a571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    0.990099\n",
            "1    0.990099\n",
            "2    0.990099\n",
            "3    1.000000\n",
            "4    0.990099\n",
            "5    1.000000\n",
            "6    1.188119\n",
            "7    1.782178\n",
            "Name: x01, dtype: float64 0    0.896552\n",
            "1    1.000000\n",
            "2    1.055172\n",
            "3    0.896552\n",
            "4    1.000000\n",
            "5    1.055172\n",
            "6    0.896552\n",
            "7    1.000000\n",
            "Name: x02, dtype: float64 0    1.009091\n",
            "1    1.000000\n",
            "2    0.993506\n",
            "3    1.009091\n",
            "4    1.000000\n",
            "5    0.993506\n",
            "6    1.009091\n",
            "7    1.000000\n",
            "Name: x03, dtype: float64 0    0.955835\n",
            "1    0.996883\n",
            "2    0.972192\n",
            "3    0.953983\n",
            "4    1.003055\n",
            "5    0.969106\n",
            "6    1.098423\n",
            "7    1.432055\n",
            "Name: y3, dtype: float64\n",
            "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0]]\n",
            "[[0.99009901 0.89655172 1.00909091]\n",
            " [0.99009901 1.         1.        ]\n",
            " [0.99009901 1.05517241 0.99350649]\n",
            " [1.         0.89655172 1.00909091]\n",
            " [0.99009901 1.         1.        ]\n",
            " [1.         1.05517241 0.99350649]\n",
            " [1.18811881 0.89655172 1.00909091]\n",
            " [1.78217822 1.         1.        ]]\n"
          ]
        }
      ],
      "source": [
        "'''>>>>> \n",
        "Keras model for comparison with first principles model'''\n",
        "\n",
        "#import useful packages\n",
        "import keras\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import keras.backend as kb\n",
        "import tensorflow as tf\n",
        "#the follwoing 2 lines are only needed for Mac OS machines\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "#raw data in dictionary form x01, x02, x03, y3\n",
        "my_dict = { \n",
        "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
        "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
        "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n",
        "    'y3' : [30.97, 32.3, 31.5, 30.91, 32.5, 31.4, 35.59, 46.4]\n",
        "}\n",
        "#normalized inputs in array\n",
        "xdata = []\n",
        "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
        "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
        "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
        "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
        "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
        "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
        "xdata.append([36./20.2, 14.5/14.5, 308.0/308.0]) \n",
        "\n",
        "#data frame\n",
        "df = pd.DataFrame(my_dict)\n",
        "#devide by the median to normalize \n",
        "df.x01= df.x01/20.2\n",
        "df.x02= df.x02/14.5\n",
        "df.x03= df.x03/308.0\n",
        "#normalize output array\n",
        "df.y3= df.y3/32.401\n",
        "df.head\n",
        "print (df.x01, df.x02, df.x03, df.y3)\n",
        "\n",
        "xarray= np.array(xdata)\n",
        "print (xdata)\n",
        "print (xarray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKbIbArteRZ8",
        "outputId": "cc552f46-4697-4224-e187-8bc77c32a096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 1)\n",
            "(1, 1)\n",
            "(1, 1)\n",
            "Weights and biases of the layers before training the model: \n",
            "\n",
            "dense_one\n",
            "Weights\n",
            "Shape:  (3, 1) \n",
            " [[1.3 ]\n",
            " [0.38]\n",
            " [0.75]]\n",
            "Bias\n",
            "Shape:  (1,) \n",
            " [-0.14] \n",
            "\n",
            "dense_two\n",
            "Weights\n",
            "Shape:  (1, 1) \n",
            " [[0.75]]\n",
            "Bias\n",
            "Shape:  (1,) \n",
            " [-0.13] \n",
            "\n",
            "dense_three\n",
            "Weights\n",
            "Shape:  (1, 1) \n",
            " [[0.8]]\n",
            "Bias\n",
            "Shape:  (1,) \n",
            " [0.011] \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.core.Dense at 0x1a475347f08>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x1a475347288>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x1a475334a48>]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define model\n",
        "\n",
        "#As seen below, we have created three dense layers each with just one neuron. \n",
        "#A dense layer is a layer in neural network that’s fully connected. \n",
        "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
        "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
        "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
        "\n",
        "from keras import backend as K\n",
        "#initialize weights with values between -0.2 and 1.2\n",
        "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
        "\n",
        "# define three layer model with one neuron in each layer\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(1, activation=K.elu, input_shape=[3],  kernel_initializer=initializer, name=\"dense_one\"),\n",
        "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_two\"),\n",
        "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_three\")\n",
        "  ])\n",
        "\n",
        "\n",
        "#set starting values to those used in first principles model\n",
        "#w01n =  1.2214114647164782\n",
        "#w02n =  0.3889371041283475\n",
        "#w03n =  0.688983355933903\n",
        "#b1n =  -0.16101132060984205\n",
        "#w12n =  0.7168491242083397\n",
        "#b2n =  -0.12791046679526907\n",
        "#w23n =  0.6967103560817965\n",
        "#b3n =  0.004475944730426142\n",
        "\n",
        "w01n =  1.3\n",
        "w02n =  0.38 \n",
        "w03n =  0.75\n",
        "b1n =  -0.14\n",
        "w12n =  0.75\n",
        "b2n =  -0.13\n",
        "w23n =  0.8\n",
        "b3n =  0.011\n",
        "\n",
        "weights0 =  [[ w01n], [w02n], [ w03n]]\n",
        "w0array= np.array(weights0)\n",
        "print(np.shape(w0array))\n",
        "bias0 = [b1n]\n",
        "bias0array= np.array(bias0)\n",
        "L0=[]\n",
        "L0.append(w0array)\n",
        "L0.append(bias0array)\n",
        "model.layers[0].set_weights(L0) \n",
        "\n",
        "weights1 =  [[ w12n]]\n",
        "w1array= np.array(weights1)\n",
        "print(np.shape(w1array))\n",
        "bias1 = [b2n]\n",
        "bias1array= np.array(bias1)\n",
        "L1=[]\n",
        "L1.append(w1array)\n",
        "L1.append(bias1array)\n",
        "model.layers[1].set_weights(L1)\n",
        "\n",
        "weights2 =  [[ w23n]]\n",
        "w2array= np.array(weights2)\n",
        "print(np.shape(w2array))\n",
        "bias2 = [b3n]\n",
        "bias2array= np.array(bias2)\n",
        "L2=[]\n",
        "L2.append(w2array)\n",
        "L2.append(bias2array)\n",
        "model.layers[2].set_weights(L2)\n",
        "\n",
        "\n",
        "print(\"Weights and biases of the layers before training the model: \\n\")\n",
        "for layer in model.layers:\n",
        "  print(layer.name)\n",
        "  print(\"Weights\")\n",
        "  print(\"Shape: \",layer.get_weights()[0].shape,'\\n',layer.get_weights()[0])\n",
        "  print(\"Bias\")\n",
        "  print(\"Shape: \",layer.get_weights()[1].shape,'\\n',layer.get_weights()[1],'\\n')\n",
        "model.layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiCldetYeRZ9"
      },
      "outputs": [],
      "source": [
        "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
        "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
        "#RMSprop is an optimizer that’s reliable and fast.\n",
        "#We’re compiling the mode using the model.compile function. The loss function used here \n",
        "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
        "\n",
        "#Running model.fit successive times extends the calculation to addtional epochs.\n",
        "\n",
        "rms = keras.optimizers.RMSprop(0.0035)\n",
        "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw0s-3HCeRZ9",
        "outputId": "a3f827df-4487-417d-f029-e7a43ff79c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
            "Epoch 2/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0175\n",
            "Epoch 3/400\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0141\n",
            "Epoch 4/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 5/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
            "Epoch 6/400\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 7/400\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0140\n",
            "Epoch 8/400\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0183\n",
            "Epoch 9/400\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0139\n",
            "Epoch 10/400\n",
            "1/1 [==============================] - 0s 996us/step - loss: 0.0185\n",
            "Epoch 11/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0138\n",
            "Epoch 12/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0188\n",
            "Epoch 13/400\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0138\n",
            "Epoch 14/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0144\n",
            "Epoch 15/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0257\n",
            "Epoch 16/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0199\n",
            "Epoch 17/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0276\n",
            "Epoch 18/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0143\n",
            "Epoch 19/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0193\n",
            "Epoch 20/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0198\n",
            "Epoch 21/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0165\n",
            "Epoch 22/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0145\n",
            "Epoch 23/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0197\n",
            "Epoch 24/400\n",
            "1/1 [==============================] - 0s 990us/step - loss: 0.0162\n",
            "Epoch 25/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0156\n",
            "Epoch 26/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0195\n",
            "Epoch 27/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0158\n",
            "Epoch 28/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0168\n",
            "Epoch 29/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
            "Epoch 30/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0154\n",
            "Epoch 31/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0180\n",
            "Epoch 32/400\n",
            "1/1 [==============================] - 0s 994us/step - loss: 0.0226\n",
            "Epoch 33/400\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0146\n",
            "Epoch 34/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0194\n",
            "Epoch 35/400\n",
            "1/1 [==============================] - 0s 996us/step - loss: 0.0206\n",
            "Epoch 36/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 37/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0143\n",
            "Epoch 38/400\n",
            "1/1 [==============================] - 0s 999us/step - loss: 0.0173\n",
            "Epoch 39/400\n",
            "1/1 [==============================] - 0s 996us/step - loss: 0.0143\n",
            "Epoch 40/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0175\n",
            "Epoch 41/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0142\n",
            "Epoch 42/400\n",
            "1/1 [==============================] - 0s 993us/step - loss: 0.0178\n",
            "Epoch 43/400\n",
            "1/1 [==============================] - 0s 999us/step - loss: 0.0142\n",
            "Epoch 44/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0181\n",
            "Epoch 45/400\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0141\n",
            "Epoch 46/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0184\n",
            "Epoch 47/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0140\n",
            "Epoch 48/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 49/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
            "Epoch 50/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0189\n",
            "Epoch 51/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0138\n",
            "Epoch 52/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
            "Epoch 53/400\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0138\n",
            "Epoch 54/400\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0290\n",
            "Epoch 55/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0262\n",
            "Epoch 56/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0155\n",
            "Epoch 57/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0195\n",
            "Epoch 58/400\n",
            "1/1 [==============================] - 0s 995us/step - loss: 0.0159\n",
            "Epoch 59/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0166\n",
            "Epoch 60/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0193\n",
            "Epoch 61/400\n",
            "1/1 [==============================] - 0s 991us/step - loss: 0.0154\n",
            "Epoch 62/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0178\n",
            "Epoch 63/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0226\n",
            "Epoch 64/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0147\n",
            "Epoch 65/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0191\n",
            "Epoch 66/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0206\n",
            "Epoch 67/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 68/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
            "Epoch 69/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0173\n",
            "Epoch 70/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
            "Epoch 71/400\n",
            "1/1 [==============================] - 0s 988us/step - loss: 0.0176\n",
            "Epoch 72/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0141\n",
            "Epoch 73/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0179\n",
            "Epoch 74/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0140\n",
            "Epoch 75/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0181\n",
            "Epoch 76/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0140\n",
            "Epoch 77/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0184\n",
            "Epoch 78/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0139\n",
            "Epoch 79/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0187\n",
            "Epoch 80/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0138\n",
            "Epoch 81/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0189\n",
            "Epoch 82/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0138\n",
            "Epoch 83/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0286\n",
            "Epoch 84/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0259\n",
            "Epoch 85/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0154\n",
            "Epoch 86/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0195\n",
            "Epoch 87/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0159\n",
            "Epoch 88/400\n",
            "1/1 [==============================] - 0s 996us/step - loss: 0.0165\n",
            "Epoch 89/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0193\n",
            "Epoch 90/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0155\n",
            "Epoch 91/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0177\n",
            "Epoch 92/400\n",
            "1/1 [==============================] - 0s 999us/step - loss: 0.0190\n",
            "Epoch 93/400\n",
            "1/1 [==============================] - 0s 994us/step - loss: 0.0150\n",
            "Epoch 94/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0194\n",
            "Epoch 95/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0278\n",
            "Epoch 96/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0162\n",
            "Epoch 97/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0180\n",
            "Epoch 98/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0150\n",
            "Epoch 99/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 100/400\n",
            "1/1 [==============================] - 0s 990us/step - loss: 0.0180\n",
            "Epoch 101/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0146\n",
            "Epoch 102/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 103/400\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 997us/step - loss: 0.0204\n",
            "Epoch 104/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0170\n",
            "Epoch 105/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
            "Epoch 106/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0172\n",
            "Epoch 107/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
            "Epoch 108/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0175\n",
            "Epoch 109/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
            "Epoch 110/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 111/400\n",
            "1/1 [==============================] - 0s 999us/step - loss: 0.0139\n",
            "Epoch 112/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0180\n",
            "Epoch 113/400\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0139\n",
            "Epoch 114/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0183\n",
            "Epoch 115/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0138\n",
            "Epoch 116/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0186\n",
            "Epoch 117/400\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0137\n",
            "Epoch 118/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0144\n",
            "Epoch 119/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0260\n",
            "Epoch 120/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0279\n",
            "Epoch 121/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0183\n",
            "Epoch 122/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0200\n",
            "Epoch 123/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0168\n",
            "Epoch 124/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0139\n",
            "Epoch 125/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0199\n",
            "Epoch 126/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0164\n",
            "Epoch 127/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0150\n",
            "Epoch 128/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0197\n",
            "Epoch 129/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0161\n",
            "Epoch 130/400\n",
            "1/1 [==============================] - 0s 996us/step - loss: 0.0162\n",
            "Epoch 131/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0195\n",
            "Epoch 132/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0156\n",
            "Epoch 133/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 134/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0192\n",
            "Epoch 135/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0152\n",
            "Epoch 136/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0189\n",
            "Epoch 137/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0224\n",
            "Epoch 138/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0144\n",
            "Epoch 139/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0204\n",
            "Epoch 140/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0251\n",
            "Epoch 141/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0139\n",
            "Epoch 142/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0143\n",
            "Epoch 143/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0192\n",
            "Epoch 144/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0198\n",
            "Epoch 145/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0165\n",
            "Epoch 146/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0145\n",
            "Epoch 147/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0197\n",
            "Epoch 148/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0162\n",
            "Epoch 149/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0156\n",
            "Epoch 150/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0195\n",
            "Epoch 151/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0158\n",
            "Epoch 152/400\n",
            "1/1 [==============================] - 0s 999us/step - loss: 0.0168\n",
            "Epoch 153/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0193\n",
            "Epoch 154/400\n",
            "1/1 [==============================] - 0s 996us/step - loss: 0.0154\n",
            "Epoch 155/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0179\n",
            "Epoch 156/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0190\n",
            "Epoch 157/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0149\n",
            "Epoch 158/400\n",
            "1/1 [==============================] - 0s 996us/step - loss: 0.0197\n",
            "Epoch 159/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0279\n",
            "Epoch 160/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0164\n",
            "Epoch 161/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0180\n",
            "Epoch 162/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0149\n",
            "Epoch 163/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0174\n",
            "Epoch 164/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0179\n",
            "Epoch 165/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 166/400\n",
            "1/1 [==============================] - 0s 999us/step - loss: 0.0187\n",
            "Epoch 167/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0203\n",
            "Epoch 168/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0169\n",
            "Epoch 169/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0141\n",
            "Epoch 170/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0171\n",
            "Epoch 171/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0141\n",
            "Epoch 172/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0174\n",
            "Epoch 173/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
            "Epoch 174/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0177\n",
            "Epoch 175/400\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0140\n",
            "Epoch 176/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0180\n",
            "Epoch 177/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0140\n",
            "Epoch 178/400\n",
            "1/1 [==============================] - 0s 996us/step - loss: 0.0182\n",
            "Epoch 179/400\n",
            "1/1 [==============================] - 0s 993us/step - loss: 0.0139\n",
            "Epoch 180/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0143\n",
            "Epoch 181/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0256\n",
            "Epoch 182/400\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.0273\n",
            "Epoch 183/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0142\n",
            "Epoch 184/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0200\n",
            "Epoch 185/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0164\n",
            "Epoch 186/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0153\n",
            "Epoch 187/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0198\n",
            "Epoch 188/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0160\n",
            "Epoch 189/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0165\n",
            "Epoch 190/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0195\n",
            "Epoch 191/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0156\n",
            "Epoch 192/400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 193/400\n",
            "1/1 [==============================] - 0s 999us/step - loss: 0.0192\n",
            "Epoch 194/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0151\n",
            "Epoch 195/400\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.0192\n",
            "Epoch 196/400\n",
            "1/1 [==============================] - 0s 997us/step - loss: 0.0224\n",
            "Epoch 197/400\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0143Restoring model weights from the end of the best epoch.\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0143\n",
            "Epoch 00197: early stopping\n",
            "best epoch =  117\n",
            "smallest loss = 0.01374325156211853\n"
          ]
        }
      ],
      "source": [
        "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
        "#I started with epochs value of 100 and then tested the model after training. \n",
        "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
        "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
        "#I found acceptable prediction accuracy.\n",
        "\n",
        "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
        "#During model training, if all the batches of data are seen by the model once, \n",
        "#we say that one epoch has been completed.\n",
        "\n",
        "# Add an early stopping callback\n",
        "es = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='loss', \n",
        "    mode='min', \n",
        "    patience = 80, \n",
        "    restore_best_weights = True, \n",
        "    verbose=1)\n",
        "# Add a checkpoint where loss is minimum, and save that model\n",
        "mc = tf.keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
        "                     mode='min',  verbose=1, save_best_only=True)\n",
        "\n",
        "historyData = model.fit(xarray,df.y3,epochs=400,callbacks=[es])\n",
        "\n",
        "loss_hist = historyData.history['loss']\n",
        "#The above line will return a dictionary, access it's info like this:\n",
        "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
        "print ('best epoch = ', best_epoch)\n",
        "print('smallest loss =', np.min(loss_hist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHJyvGu9eRZ-",
        "outputId": "33e2b569-e3f7-4d62-8a01-d6d8a39d80cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.2167306 ]\n",
            " [0.29071605]\n",
            " [0.7192791 ]]\n",
            "w01 =  1.2167306 w02 =  0.29071605 w03 =  0.7192791\n",
            "[-0.14369337]\n",
            "b1 =  [-0.14369337]\n",
            "[[0.7075463]]\n",
            "w12 =  0.7075463\n",
            "[-0.10284577]\n",
            "b2 =  [-0.10284577]\n",
            "[[0.6809736]]\n",
            "w23 =  0.6809736\n",
            "[0.03756597]\n",
            "b3 =  [0.03756597]\n",
            "x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n",
            "0.9900990099009901 0.896551724137931 1.009090909090909 0.9558346964599856 [[0.95403385]]\n",
            "0.9900990099009901 1.0 1.0 0.9968828122588808 [[0.9653737]]\n",
            "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9721922162896206 [[0.97085136]]\n",
            "1.0 0.896551724137931 1.009090909090909 0.9539829017622912 [[0.9598383]]\n",
            "0.9900990099009901 1.0 1.0 1.003055461251196 [[0.9653737]]\n",
            "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.9766558]]\n",
            "1.188118811881188 0.896551724137931 1.009090909090909 1.0984228881824636 [[1.0701221]]\n",
            "1.7821782178217822 1.0 1.0 1.4320545662170918 [[1.4297266]]\n",
            "  \n",
            "x01,  x02,   x03,  y3,  a3*32.4:\n",
            "20.0 13.0 310.8 30.969044165303533 [[30.910698]]\n",
            "20.0 14.5 308.0 32.29900311718774 [[31.278109]]\n",
            "20.0 15.3 306.0 31.499027807783705 [[31.455585]]\n",
            "20.2 13.0 310.8 30.909046017098234 [[31.09876]]\n",
            "20.0 14.5 308.0 32.498996944538746 [[31.278109]]\n",
            "20.2 15.3 306.0 31.3990308941082 [[31.643648]]\n",
            "23.999999999999996 13.0 310.8 35.58890157711182 [[34.67196]]\n",
            "36.0 14.5 308.0 46.398567945433776 [[46.323143]]\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "#For results of training network:\n",
        "\n",
        "#keras.layer.get_weights() function retrieves weight values\n",
        "first_layer_weights = model.layers[0].get_weights()[0]\n",
        "w01 = first_layer_weights[0][0]\n",
        "w02 = first_layer_weights[1][0]\n",
        "w03 = first_layer_weights[2][0]\n",
        "first_layer_bias  = model.layers[0].get_weights()[1]\n",
        "b1 = first_layer_bias\n",
        "second_layer_weights = model.layers[1].get_weights()[0]\n",
        "w12 = second_layer_weights[0][0]\n",
        "second_layer_bias  = model.layers[1].get_weights()[1]\n",
        "b2 = second_layer_bias\n",
        "third_layer_weights = model.layers[2].get_weights()[0]\n",
        "w23 = third_layer_weights[0][0]\n",
        "third_layer_bias  = model.layers[2].get_weights()[1]\n",
        "b3 = third_layer_bias\n",
        "\n",
        "#print weights and biases\n",
        "print (first_layer_weights)\n",
        "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
        "print (first_layer_bias)\n",
        "print ('b1 = ', b1)\n",
        "print (second_layer_weights)\n",
        "print ('w12 = ', w12)\n",
        "print (second_layer_bias)\n",
        "print ('b2 = ', b2)\n",
        "print (third_layer_weights)\n",
        "print ('w23 = ', w23)\n",
        "print (third_layer_bias)\n",
        "print ('b3 = ', b3)\n",
        "\n",
        "#use model.predict() function to print model predictions for data conditions\n",
        "xarray= np.array(xdata)\n",
        "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
        "test = []\n",
        "for i in range(0,8): \n",
        "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
        "    testarray = np.array(test)\n",
        "    a3 = model.predict(testarray)\n",
        "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
        "print('  ')\n",
        "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
        "for i in range(0,8): \n",
        "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
        "    testarray = np.array(test)\n",
        "    a3 = model.predict(testarray)\n",
        "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKux2AnOeRZ-",
        "outputId": "eb54eff2-2fbb-42ec-ff4d-04d308298b27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[31.026031044602398, 31.392252068936827, 31.565112471222882, 31.213962922632696, 31.392252068936827, 31.75304434925318, 34.784753580212595, 46.427124830126765]\n",
            "[30.97, 32.3, 31.5, 30.91, 32.5, 31.4, 35.59, 46.4]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_data=[]\n",
        "y_predicted=[]\n",
        "\n",
        "for k in (df.y3):\n",
        "    y_data.append(k*32.401)\n",
        "    \n",
        "for i in range(0,8):\n",
        "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
        "    testarray = np.array(test)\n",
        "    a3 = model.predict(testarray)\n",
        "    y_predicted.append(a3[0][0]*32.401)\n",
        "    \n",
        "print (y_predicted)\n",
        "    \n",
        "print (y_data)\n",
        "\n",
        "plt.figure()\n",
        "plt.loglog(sorted(y_predicted),sorted(y_data))\n",
        "plt.xlabel('ydata')\n",
        "plt.ylabel('ypredicted')\n",
        "plt.title('log evolution with keras model of ypredicted as a function of ydata')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4ThmnINeRZ-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}